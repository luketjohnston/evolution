IDEA: 
generate completely random neural networks until we get a high-performing one on mnist.
If it generalizes, then we can say that gradient descent has no effect on generalization ability,
and instead it is probably due to the "number of generalizing networks" for 
certain data distributions.
One way to think about this would be compression - the maximally-compressed network has the most
"free variables" and hence takes up the most volume in the high-dimensional manifold of its parameters.

TODO: try, instead of finding the best mutation by evaling each mutation on the full trainset,
just randomly mutating and choosing the one that has highest cosing similarity with the gradient. See if it 
performs better / worse / same as full GA.

TODO include hyperparams in save .pkl 

List of main experiment ideas:

| TODO: try adding some loss term that enforces a particular order on the features learned,
| then see if we can interpolate between different networks trained with that norm
| to test if it works. Toward the eventual goal of allowing multiple modules to learn
| and interface together.
| 
| Add biases to GA mutation, figure out right mutation schemes
| 
| Continue to try smaller LR mutation rates to find the optimal one
| 
| Try LR mutation with a smaller generation (or fixed with higher generation) to isolate effects of each
| 
| Experiment with different types of mutation again to verify that exponential is performing better than normal
| 
| Experiment with different values for lambda in the exponential network
| 
| Instead of mutating parameters directly, mutate how they change with each minibatch (a hebbian rule, for example)

The really long run is NOT a multi, it is misnamed

3/13/24: working on adding sexual reproduction. Should speed up convergence since now beneficial mutations can occurr in parallel
(and be recombined into the same chromosome by sexual reproduction).
To compute the likelihood of two beneficial mutations A and B combining in the immediately following generation, you can do the math
like here:
https://www.wolframalpha.com/input?i=plot+1-%28%28x*%28x-1%29-2%29%2F%28x*%28x-1%29%29+%2B+%282+%2F+%28x*%28x-1%29%29%29*%283%2F4%29%29%5Ec.+let+c+%3D+512.+between+x+%3D+2+and+x+%3D+128
the 3/4 comes from the chance that, given parents A and B, a child does not receive both A and B (1/2 * 1/2)
ACTUALLY: on second thought, I don't think this math is important. There is a competing effect - the above shows that the fewer parents
in a generation, the more likely that two beneficial mutations will be immediately combined. BUT the opposite effect is that it is ALSO more likely
for a single beneficial mutation to be overwrote entirely by another beneficial one (A has 4 kids that are all better than B's kids, because A and B 
never combined, and there are only 4 parents, so A makes all 4 parents of the next generation). If there are MORE parents per generation,
then it is less likely that a beneficial mutation will go extinct before it can recombine with other better ones.
Can always just test this stuff experimentally anyway...


Current best idea:
test the hypothesis that GA mutations interfere with previous knowledge by memorizing 
some random dataset.

With these params:
input_dims = [64,64,3]
num_classes = 10
batch_size = 32
num_train_datapoints = 16 * batch_size
num_val_datapoints = 2 * batch_size
kernels = [8,4,3]
channels = [3,32,64,64]
strides = [4,2,1]
hidden_size = 512
adam gets to 1e-5 within ~100 epochs on MemorizationDataset


Hypothesis:
randomly mutating the entire network every time makes it difficult to develop complicated machinery,
since as we build up skills, every mutation has a chance of interfering with all previously built up 
skills.
We need a way for mutations on average to be benign, or at least a way for mutations to not
"overwrite" whatever skills we already have
Rapidfire brainstorming::
-Mutations apply to only one layer (skip connections)
-mutations add layers one at a time
-mutations add separate models that are ensembled together
-there is an attention layer of some kind that can attend to different mutation layers
-Setup the random weights such that layers on average make small changes
-each mutation is a totally separate model that additionally outputs an "importance" 
channel, and at every step we use the model that outputs the highest importance. This last one
feels somewhat physical, we have all these different desires and they often compete, and presumably
the most pressing one wins. The alternative idea is the central system has its own coherent 
narrative that it takes into account all subsystems and then choses.
Some problems:
1. how do we mutate the final layer?
--- Incredibly it seems like genetic algorithms may provide a solution to this already - 
over time, chromosomes that are most likely to be robust to mutation with respect to the fitness
function will out-compete others. Then, only the unimportant elements will mutate and hence discover
new fronteirs


Roms are installed: /Users/luke/Library/Python/3.9/lib/python/site-packages/AutoROM/roms


If I get this error when using eksctl:
Error: checking AWS STS access â€“ cannot get role ARN for current session: operation error STS: GetCallerIdentity, https response error StatusCode: 0, RequestID: , request send failed, Post "https://sts..amazonaws.com/": dial tcp: lookup sts..amazonaws.com: no such host
If I just specify the region to be us-west-2, then it works again.
TODO investigate


TODO: 
the "frames" reported in the GA paper are "game frames", not "training frames". 
So if we train on every 4th frame, make sure we are incrementing frame count by 4 each training
step. Not sure if I am doing this right now.

TODO: havent figured out how to connect to rabbitmq UI


52.52 was the last price before I stopped everything


Things to think about
1. indirect encoding - how can we ensure that beneficial mutations are re-used everywhere they are applicable
in the network etc?
2. Sexual selection - how can agents choose their mates?
3. Diversity - how can we enforce some diversity constraints?
   --- there may be a way to enforce this by penalizing the opposite - 
   organism that are too non-diverse die out
4. think about global competition - in real life, organism succeed by finding NEW niches, not by
dominating an existing one (diversifying). one idea is to compete *locally*, vs only similar solutions




The current network is 3 convolutions with channels 32,64,64
and strides 4,2,1
and then a MLP.

Suppose that our model consists of multiple sub-components, which can evolve independently, but which
are ensembled together at the end in some manner. 
How are we to ensemble them together?
Three possibilities I can think of so far:
1. each model outputs an action directly, and these actions are ensembled somehow
2. each model outputs only a state, which is ensembled somehow into an action by a final layer
   that itself must evolve independently (or possible is fixed in some way)
3. Each model outputs both a action AND a state, but somehow we use both or only the state.

Another idea: I could evolve to a different objective (modeling the environment, for example), and
then setup different components myself to each evolve (rather than requiring the multiple components to 
evolve to their individual tasks from scratch)
   - then 

The "final layer" must be something constant, that inherently is able to "attend" to the ensemble.
Then we can evolve each ensemble

Alright here we go:
There are evolved state representations and evolved reward functions, and a meta-controller.
State representations and reward functions are fixed for a given agent.
An agent is trained on multiple episodes, during which the meta-controller learns to use the state
representations to maximize reward. Meta-controller could be for example an RL loop



This is all too complicated. The hypothesis that I actually want to test is 
1. do genetic algorithms as they currently exist reach a "diminishing returns" point whereby further mutations
   interfere with previous learnings.


Current issue is that each generation takes successively longer to inialize the policy. 
This is a bottleneck for the memorization dataset, it may not be the bottleneck for a larger task 
(at generation 500 it takes 2 minutes to initialize the policy). No matter how many workers we scale up
to, whatever percent of the work-time it takes to initialize the policy will always take up that same
amount of resources (money or time). Possible solutions:
1. try to speed up the initialization process somehow
   - paper solution: pre-generate random matrix which we then index into? I'm not sure how this would be faster
2. try to prevent full re-initialization when possible? 
   - each worker can cache some set of networks, and then only has to update the network with just the new mutation.
     -- difficulty: how much space do networks take? can we store multiple on a worker?
     -- maybe we don't have to, if the master thread can organize the tasks so that workers usually receive 
        individuals that they already have cached. Seems tricky
3. change the way in which networks mutate, to be faster.
   - instead of mutating entire network, mutate one layer? reduces time by factor of (network depth). can we do this without
   increasing total generations required (or possibly even decreasing it?)
^^^ THE ABOVE IS SOLVED ^^^ (for some modules, not all)


Random (not really related) thought: What if where was a way to enforce a standard ordering on network parameters?
For example, order nodes by sum of incoming weights or something like that. Would this make it easier to mutate
parameters one layer at a time?
- there are ways, see Keller's paper and git re-basin. They all require some computation though.


Observation:
When we learn the no-grad memorization task, if we simply use the optimal settings for the normal 
memorization task (1 parent sigma 0.02 child 16), learning barely progresses at all. Increasing sigma
alone doesn't fix the problem, learning halts at around 21. However if we increase the number of parents
(and children) to 8(32), then learning continues to 15 at generation 500.
Hypothesis:
Something about this setup incentivizes parents to become robust to mutation with respect to the 
information they have already learned. Children of the parent that reproduces the highest average fitness
will be the most likely to populate the next generation. This means that parents that are "robust"
to mutation with respect to whatever they have already learned will be more likely to persist. However,
there is no incentive for them to develop robustness to mutation in generall - only robustness with
respect to the datapoints they have already memorized. So mutations will be able to continue
exploring new ground without overwriting previous accomplishments.



Possible next steps:
1. experiment with different architectures+mutation schemes, to find one that can solve the nograd memorization
problem quickly. A simple idea is just a NN thing where we mutate the 10 reference points, for example. 
3. run a bunch more experiments to see if the current setup can actually solve the nograd memorization problem
with some set of parameters
4. try to falsify our hypothesis that multiple parents allow multi-generation selection effects where 
chromosomes that are robust to mutation with respect to the current fitness level are more likely to survive
and propogate
5. Mutate mutation rate per parameter

What if we evolved a rule whereby connections between neurons were strengthened / weakened over time?
let's say fixed kernel size of 4: 
Neural network takes as input:
-- one window of the input (4x4xc)
-- current weights connecting each input to the output
-- (optional: current output activation)
And outputs:
-- updates to each weight
Could have other networks in there, for example one that takes as input all the channels at a given
location over multiple input images, and outputs an adjustment to all input weights etc

Random idea:
there are two elements of the brain.
The thing that senses
The thing that acts

-- If the thing that acts, can learn to operate with arbitrary types of senses, then the thing that senses
can evolve independently to just sense better (add more vision neurons, add more abstraction, add "memories" 
to the sensation if visual input triggers something encountered before, etc)
-- the thing that acts just has to be able to act with arbitrary sensual input (which it clearly must adapt
to over time, since it is arbitrary).


Random papers:
https://www.nature.com/articles/s41567-023-02332-9
-- mathematical modeling of both hebbian plasticy and random modifications reproduces natural 
   connection-strength distributions and clustering
-- in real brains, "fire-together-wire-together" is asymmetric and has time dependency. This paper
   doesn't look into that


We need a way for the genetic algorithm to learn which parameters to continue mutating and which parameters to 
stop mutating (This would also allow learning of the learning rate). 
Let's say we have n parameters, and we also keep track of n "mutation rate"s. 
Network mutatino would be p += normal(shape(p)) * sigma * (mutation rate)
The question is, how does mutation rate change?

I have this interesting idea that sometimes, mutations that CAUSE death could be beneficial to the population. Think
for example of this mutation rate above. For some parameters, we will want it to become very low, and then stay there.
But once it is low enough, there will no longer be any selective pressure on it - so it will random walk until it is high
again, and then it will take a lot of mutations to make it sufficiently low again. But, if the population were to develop
another mutation that causes future children to die if their mutation rate increases above a low threshold, then
it would be possible to NEVER see that parameter change again. 

Put more simply - if a mutation is slightly bad, it is to the population's advantage to weed it out early, before it 
randomly propogates (and consumes resources). This could be accomplished with some form of sexual selection also. ***

I think the first thing we should try is just the most basic "mutating learning rate". Keep a lr for each parameter, mutate
it at some constant rate, pass it through some function that maps it to [0,inf] (probably e^x), and then use it to mutate. Let's do it!


So far, attempts at variable learning rates have not allowed me to succesfully memorize 64 datapoints with the "tilldeath" loss.
One thought is that the population size is too small? If larger pop size, then non-harmful or slightly-harmful mutations
can persist longer before they are outcompeted and rendered extinct. Or just, there is more diversity in a given pop.
This doesn't really seem correct though - I don't want to just make it more likely that random chance will get us there,
I want to be able to learn what changes actually will.

What if we add some momentum to updates? If a param has been updated in one direction for the past few steps, then continue moving 
in that direction

Another thought is to add stochasticity into the actions - the loss can still be deterministic based on what actions are taken,
but this would allow agents with a higher likelihood for the correct action to be more likely to survive, and hence
provide some selective pressure to move in the right direction. A risk would be that too small populations will
now be dominated by luckiness in actions?

Apparently there is a "50/500 rule" in genetics (minimum viable population) that a population needs 50 individuals to prevent
inbreeding depression, and 500 to guard against genetic drift at large.

What stats should I record?
1. mean and variance of learning rate (across all params)
2. How much each "memory" is being used
3. magnitude of each "memory"
-- could just figure all this out from the save state

OK upon investigating, I found that the norm of each  memory was increasing with every mutation. Clearly this is undesireable,
increasing amounts of mutation are then required to make the same scale of changes. Possible fixes:
1. restrict the norm of each parameter to remain the same 
2. add some l2 regularization to the loss
3. multiple the mutation by the norm of the parameter
4. allow mutation size to increase in some other way
5. somehow restrict each mutation to keep the norm the same (similar to 1)
I like 1 the best, lets implement it.


For 1 though, how do we reverse the process (for efficient mutation from cached policies)?
given 
   noise
   p + noise / | p + noise |
and the knowledge that |p| = 1, how do we determine p?
- we can do it with the law of cosines, although I am not sure how to figure out the +/- in the
quadratic formula besides trial and error. Surely there is an easier way...


Alright after some random experimentation, I decided to try just mutating each "memory" to a random direction
on the unit sphere, without any regard to whatever was there previously. This learns reasonably fast (for the normal
memorization task), but not as fast as the previous memorization policy, which had the norm of each m emory increasing
with every mutation. I am not sure exactly why this is. One hypothesis is that, by adding to an existing memory,
we are making it more likely to stay "near" that direction, and random walking "near" a good direction is more likely
to uncover a better direction than random re-initialization. Another hypothesis is that it makes it possible for the memory 
to become more and more robust to mutation. I still want to figure out a more intelligent way to select which memory to mutate,
though.

The 'increasing norm' approach could have another benefit, which is that it would allow stuff like this

  +       +     +
     +   +       +
   +       -  +  
      +   +       +
    +   +   +

 to be classified with only 2 memories, if the magnitudes are selected carefully (I think). Not sure about this, TODO

Let's consider this solution again:
- memories are unit vectors
Somehow, memories determine logits for each action. The closer a memory is to a query, the higher its logit is.
Then, selection pressure at the population level will move each memory closer to the query.


What if we just scale up the number of memories a lot? Then we can randomly mutate to add in vectors that dont change much,
more likely to eventually get ones that are close to correct
TODO think about if this still has problems once we need to remember more things

The downside of this whole memory mechanism is the following:
suppose we have a bunch of dogs in an area with one cat
we can randomly mutate a couple memories in this "dog" area that classify the whole area as dog.
Then, how do we learn a single memory for the cat?
well, even if we randomly mutate to have exactly the cat vector, we may not learn it if it overwrites the 
correct "dog" classification for some other nearby vector. 
So we would be relying on random mutations to build up in this "dog" area over a long time, without any
selective pressure to do this, until it is "safe" for the cat memory to mutate because it will no longer
disturb the correct classification of any dog memories.

Experiment:
train some MemModuleBasic to -0.5 loss, which means there is a single datapoint incorrectly classified.
I tested how many memory modules are being used to classify the other 63 datapoints, and it is 63. This was initially
surprising to me, I thought there might be a couple that are classified correctly, but I think what is happening is that
none of the memories are actually that close to the datapoints, they are just closer than any other memory, and 
there are so many dimensions that it is keeps being possible to find some smaller and smaller subspace of the sphere
that is closest to the target and that doesn't interfere with any other memories. TODO this is something we could experimentally/mathematically
verify. So once we get to 63, the subspace that's left is very small (or possibly nonexistent, TODO check).
The experiment I tried here so far is to just take a memory that isn't being used, set it to exactly equal the desired target memory, and 
see what happens. What happens is that the total number of correctly classified memories drops massively from 63 to 37. 


Plan:
1. make memmodule that mutates each memory instead of totally replacing it, keeping the memory normalized
2. add some kind of intrinsic reward which rewards how close the closest memory is to the input
-> this should be enough to stop getting "stuck" at the end, I think, and learn all datapoints faster

TODO redo some experiments, I had done them with sexual instead of asexual by accident


Did the above, it works, can learn 64 with 64memheads pretty fast (500 generations was the fastest I saw)
Still occasionally will get stuck on the last step, I am guessing because each memory is only updated 1/64 * pop_size
per generation (so once for my current settings). curious what happens if we just update all the memories

Alright looking at some runs with the above and various sigmas, sigma=1 seems to be the best! Maybe sigma3 also good but 
I'm inclined to leave it at sigma 1

let's do a quick "mutate all memories at once" experiment -> doesn't seem to work very well

WE need to speed up the random mutation right now. The "add a random normal and then normalize" is difficult to invert
so I'm going to move to another scheme:
1. generate a random (invertible) rotation. We can do this for uniform rotations, not sure how to restrict them
Maybe just generate some basis set of rations (all pairs of 2 coordinate systems, seems kinda overkill and would take a long time)

Other ideas...
1. provide some mechanism for reducing the size of a chromosome ( mutations that remove old mutations )
 --- need some intrinsic incentive for this, 
     

Maybe let's just work on something else for a bit..

Tradeoff with sigma: small values of sigma allow fitness to increase faster for good memories. BUT they make it much harder for not-good
memories to improve to be useful (need to get there with a random walk, not really feasible)
-- potential solution is to sometimes mutate entirely new vectors (in addition to the small sigma mutations)

Current thoughts: could just move around in Rn space (not on sphere), but still normalize before computing distance, 
problem here is just to prevent exploding norms, could use a torus space

Also had a thought that could dynamically determine the step size for updates by looking at the moving average of recent step sizes,
and then increasing/decreasing by some power factor. The idea is if a lower learning rate is more likely to improve the
fitness, then updates that actually do improve the fitness will be more likely to have had a lower learning rate, and the
baseline will adjust


IMPORTANT OBSERVATION:
when we start with a randomly initialized memory bank, learning is much slower than when we start from 0.
I suspect this is because with a randomly initialized bank, there is a starting vector that is "close" to 
every datapoint, but the vector is probably labeled incorrectly. Then when mutating, you have to mutate
a vector that is closer than the randomly initialized vector (~1/64 chance), AND that is labeled correctly (1/10).
So only 1/640 mutations will improve things (10 generations). 
We also observe that when training from randomly initialized memories, the intrinsic fitness DECREASES over time. 
(with the random resetting of memories, not random perturbation)
This seems to mean that usually when mutating we are moving a close vector away (presumably to improve 
another vector some small amount)
What if instead of mutating target and memory at once, we mutate either or?


Basically, whenever we have an "incorrect" memory close to a datapoint, in order to improve we need to do one of two things:
1. randomly select this memory to mutate, and mutate the class to be correct
2. random mutate another memory even closer than this one (and class correct)
Basically, the closer an incorrect memory is, the harder it is to improve. there is a tradeoff here in that the faster memories
mutate, the easier it is to remove bad memories (and replace them), but the harder it is for our intrinsic fitness to take effect
and keep memories close to their correct values. Intrinsic fitness as we've defined it now actually makes this even worse,
since it moves even incorrect memories close to their datapoints.

Basically we need another intrinsic fitness which is "number of used memories." and we need to often mutate deletions.

three possibly ways to mutate:
1. select an unused memory, randomly mutate it, randomly assign a class
2. select a memory and delete it
3. select a used memory and randomly perturb it.
Eventually there could be mechanisms whereby the frequencies of each of these options can evolve too, but no need for that at the beginning.
TODO ive never tried just transmitting the full network state, it might be faster than regenerating all the time... 

Weirdly, when I look at how fast the intrinsic fitness increases with different values of sigma, the value of sigma that increases the 
intrinsic fitness the fastest is not the one that increases the overall fitness the fastest. Not sure what the hell to make of this

Alright I think it's time to just start working on the next step whatever that may be.

Brainstorming:
1. setup another toy environment, where we have to both memorize random noise as well as learn some function approx. 
i.e. learn cifar as well as memorize random.
--- downside - may take a long time to test this, we might need lots of datapoints for cifar.
Can we make a similar test that doesn't require so much data? Some small set of 32 datapoints that we know can't be
memorized by the memorization module?
--maybe a small dataset of images where we just need to count the ones or something like that and take mod 10
-- maybe something simpler, just ten "images" one for each class that can appear anywhere
-- could also just use mnist and evaluate on a subset of images each time for speed
   -> let's just start with this

TODO setup an aws instance so I can easily spin up more experiments in parallel
c7g.2xlarge seems to be similar to my macbook, cost 0.1$/hr

Can we visualize the gradient updates at every step and then see what sort of mutations could reproduce them?

What was my thought process running with higher pop... the the rational was, if we just eval on 64 datapoints every time,
the most fit individuals will always be the most lucky onesa, and luck is too likely to completely outcompete any other
beneficial mutations. In retrospect this could still be true even after increasing the pop size. It seems like if
half the pop is making it through every generation then there would still be a pretty large signal from just 
improving at the actual task rather than random chance, so not sure what's going on here...
Are there some conflicting effects?
- the higher percent of each generation that has children, the higher likelihood that "good" mutations will not
be overriden by random chance. But at the same time, the LOWER the proportion of the following generation will
contain mutations from this parent, so the more difficult it is for this beneficial mutation to survive.

This is assuming each descendant has 1 parent. If we have 2 parents per offspring, we can basically double
the number of offspring an individual can have while still keeping the pop the same size (but we half the 
influence on the offspring from each parent)
--- we could test our theories by doing GA like we are currently doing it, but also evaling on the entire set,
and then tracking which beneficial mutations get overriden by random chance over time. TODO

random idea, multiple populations evolving independently and occasionally recombining?

TODO add bias FFS, or if not you have to check that your non-bias implementation still works with gradient ascent


The question we want to answer is WHY GA gets stuck somewhere when trying to learn mnist. Experiments to try:

1. eval on larger amounts of mnist at a time. 
Result: larger amounts of eval -> better performance at convergence

2. eval on full mnist. Does it converge?

3. given a training with a sequence of mutations, eval each mutation on the full mnist set, and 
determine whether good mutations just stop being found, or if bad mutations are overwriting good
mutations at too fast a rate.

4. train with GD, make sure our architecture is all set up correctly.
6. Try to get some context for the mnist loss we are getting - what is a good loss?

5. train with GD, look at what the parameter updates look like, to gain ideas for how to mutate in GA
--- maybe simplify this to just a single fully connected layer and investigate that...

7. Check if we are generalizing at all (add a validation set on mnist)

8. We already tried lots of different values of sigma and didn't find it to be all that important


Ok let's order these. I think I want to do (4) first, it seems like the best sanity check
And I suppose there's no point in ordering the rest until I get results from 4, who knows what we might learn

Findings from (4):
1. normalization of mnist images is important (speeds up convergence a lot)
2. loss should go to 0. Currently with Adam 0.001, 4 epochs to 99% valset acc and 0.02 train loss.
With SGD lr 0.1, can take as few as 3 epochs to 99% acc and 0.03 val loss!
3. one epoch of batches of size 64 takes 937 batches, so 937 steps. So even if GA is as fast as 
SGD, it will still take about 6k generations to get to .02 (in 1k it should get to val loss of .07 and 
97% acc)

Next step is to look at the parameter updates that SGD is performing and see if we can glean anything
interesting thereby.

Investigating the gradients, they clearly fail the shapiro test for normality. What distribution(s)
should we use instead to approximate them?
Findings from (5):
gradient updates follow a loosely exponential distribution (for weights). When I approximate 
this distribution with GA, still doesn't work very well. 

Think the next step is to simplify further, go to a single MLP, and figure out the best way to do
GA (rather than just randomly using the normal distribution)

Generating ideas for ways to update:
1. update only a single layer at a time? 
2. update only a single parameter at a time? Still follow exponential distribution
3. Can actually learn an algorithm for updating the weights (complicated) 
   -- real life organisms probably do not encode all synaptic connections and weights in DNA,
      more likely they learn various rules for how to update synapses 
   how would this work: 1
   1. mutate the rule
   2. run network on a batch, use the rule to update weights for that single batch
   3. run again on that batch, see if improved after update. Organisms that improve the most
      reproduce
   4. This requires storing all the network weights
   5. But once the rule is learned, can use it in future attempts

There are two unknowns:
1. how much does the inaccurrate estimate of fitness due to one minibatch affect things?
   if we do this experiment and find that we are able to learn entirely, then what does this tell us?
   that somehow the best random mutation on a minibatch is significantly different than the gradient 
   direction for that minibatch, despite the random mutations being able to recover the gradient exactly.
   Seems unlikely - more likely that the way we are mutating is just very unlikely to uncover the correct
   gradient direction.
2. what is the best way to mutate parameters when we don't have a gradient?


Three possible failure modes:
1. It reaches a point where it stops generating updates that increase fitness. Possibly lr too high,
possibly loss landscape too sharp
2. Updates that increase fitness are being generated but not being passed down
3. updates that increase fitness are being generated and passed down, but updates that decrease fitness
are being passed down just as often.
4. I suppose a fourth possibility is that it continually increases fitness, but the increases are so 
small

1. can be tested by looking at a save, and trying to generate updates for all the seeds near the end.
If we can randomly generate one that increases fitness then we can negate 1. 
3. If we look at a late save and evaluate each mutation on the full set, and if fitness increases
and decreases, then we can verify 3
2. We can additionally look at a late save and see if we can generate an update that increases fitness
(with the params the training was using). 

We have weak evidence against (2) since when we increase the number of datapoints we eval fitness on,
it doesn't entirely fix the problem (this should make fitness on a batch more closely match global 
fitness, and hence they should be passed down)

Another thing we could try is to generate the actual gradient, and then try different sigmas with
that gradient. See if the sigma we are using is just too large at the end


OK after optimizing the eval on MNIST a lot, we can run experiments faster and this is what I've found:
- Loss does seem to approach 0 with exponential updates. Does not approach 0 with the "one" updates that I have tried so far.
- it is the case the higher sigma results in faster learning initially but slower learning later. 

-- seems like a no-brainer to implement some kind of lr mutation, to allow the algorithm to update the lr
as it goes.

-- Another idea is to actually figure out the ground truth optimal LR scheduling. How do we do this?
Do a trial run where at every fitness level, we compute the next generation at a few different learning
rates, and then we save the lr that has the best performing child. Then we can setup a lr schedule,
run a GA with that lr schedule, and use that as our baseline when implementing the lr mutation.
Let's just bite the bullet and implement this thing

Problem with the sigma updating is that it makes my efficient network reconstruction not work very well.

Initial results from the LR mutation show it to do better than most other schedules, at least early.
It doesn't do *that* much better than just keeping a constant 0.03 lr. And I am also curious if even 
lower constant learning rates will eventually surpass it

Another experiment we also need to try is trying the lambda mutation with the same generation size. Because
right now I'm using a higher generation count, which could explain why the curve is better.
Alternatively, try a fixed sigma at the higher parent count.

Hypothesis: the sigma mutation as I have implemented is decreasing sigma too quickly.
Supporting evidence: a constant .03 sigma converges much slower at the beginning, but faster at the end,
than my sigma mutation method.
Are there any other alternative possibilities? Sigma mutation could be optimal and just random variance in runs
causes this. Can just run a few trials
I think we can just assume this hypothesis is true for now. What we need to figure out is WHY it is decreasing
too quickly. 
Hypotheses:
1. The loss landscape is generally very sharp, with a few directions sharply increasing for a large update,
many more moderately increasing for small updates only, and most decreasing. This means that the optimal strategy
is often to just wait and keep rerolling updates until we hit the correct direction, and we want to have the largest
possibly magnitude when we do hit it. However in practice what will happen is that if we don't hit the best update in 
one generation, then the offspring with lower mutation rates will dominate.
--- test: plot the distribution of fitness levels after a single mutation at various learning rates for a given save.
          We should be able to verify emperically pretty easily if this is what is going on.

Idea - examine the distribution of weights connected to single neuron on a fully trained network

Something doesn't line up here - when I look at the distribution of fitness changes for a single 
mutation, they are almost entirely negative. But when I look at the average fitness graph,
it just goes straight up. Even best fitness just goes straight up, when only working with
a generation size of 16. So 1/16 mutations must increase fitness

I thought maybe I wasn't loading the elite, but this isn't the case...
Double check config? Make sure I'm mutating the right way still?
I did change the mutate code, but shouldnt have changed anything, just made it a method

We can try training full runs at various LR, and then check how the fitness changes for various lr
at set fitness levels for each one (see if loss landscape looks significantly different
for each LR)

Not sure what I was doing wrong yesterday, but when I fixed everything so that it worked with 
MP, now we are getting enough improvements to supply the monotonic improvement (when we use a low enough
learning rate)

Idea I had when waking up: we can just try the beginnign of a LOT of runs, to see how much
the seed affects things (dont have to run all the way to 50k when using the same parameters)


FIRST POST:
Genetic algorithms on MNIST 
Exploring genetic algorithms to evolve function approximators.

First, we show results for training on MNIST with SGD as a sort of baseline. Inspiration from this post
for training fast, but we have used SGD to keep it simple. Still converges to 99% very vast.


Now, let's see how genetic algorithms do. We begin by asexually evolving a population of 64 individuals, 
where the fitness is the cross entropy loss on the full MNIST training data. However to even begin, 
we need to figure out how we are going to mutate the parameters of the MLP. 



TODOs:
0. Add validation acc to tensorboard
1. Optimal generation size - loss improvement analysis
2. Making it work with smaller batch size


One thing we need to do right away: add an initial phase to learning where we tune the lr 
without actually mutating the network. How do we do this? 
-- lr_only generations param to the network where for the first lr_only generations, we only
   mutate the learning rate.


What do we actually want to do?
Initialize neural network randomly a bunch of times,
evalutae each random initialization,
mutate with a given learning rate multiplied or divided by some scaling factor,
evaluate the new mutation improvement,
set new base lr to be the one with the best improvement


The problem is none of this actually solves the problem that the way I am mutating the learning rate right now
does not give me the optimal learning rate for evolution.

If I just increase the population size, then the current mutation scheme will give me close to the optimal learning rate.
./co

With small pop, elites important since sometimes fitness does not improve at all
With large pop, can remove elites entirely

What if we keep the small pop but we slow down mutation rate for sigma?
Then a larger sigma has more chance to find a good mutation before being outcompeted by more consistent
smaller sigmas...


TODO: fix loss computation (divide by num examples not num batches) after we've matched performance

lambda89 IS importnat! need to investigate...
Probably what is going on here is that my initial sigma mutation only only works well when the starting sigma is smaller
than the optimal sigma. May need to actually test this hypothesis

When staring with sigma 0.5 and exponential, and 500 sigma only generations, performance is not optimal. 
Probably sigma is still too high here.



TODO
There is something I don't understand about the variance of training runs or some setting that is causing things to 
perform much differently than expected. Are the best runs just chance? 

TODO 
perhaps exponential does worse simply because there is a higher variance in the magnitude of the update - 
try normalizing before updating. 
Some quick spamming in the interpreter says this is unlikely. The param size is (28*28)*128 for l1 and (128*10) for l2 anyway

Try visualizing the distribution of the updates actually taken by GA... this seems insane it's not going to look anything
except normal

Try visualizing the improvement distribution for different types of updates (normal vs exponential), see if any information 
can be gleaned from that

Another thing I haven't looked at is if the true gradient with respect to the entire training distribution matches the
form of the smaller batches

Unknowns:
1. how much variance is there really in a single run with good starting parameters?
2. What are the best sigma mutation parameters
3. If we initialized sigma1 and sigma2 differently could we get better performance?
4. We need to establish 1 before determining if exponential truly is worse than normal

What we have neglecting is that we need more than just "one trial fast". We need "enough trials
to ignore natural variance" fast. 

Catch 22 - we want to run a bunch of trials to see what the variance is, 
but we want to run these with the best parameters
but we can't figure out the best parameters when there's so much variance

Looking at the best performing trials, they don't seem to be the ones with the best initializations
There is so much randomness involved 

I have discovered that my initial sigma mutations does not result in the optimal starting sigma. Now, there are a few levers we could try to fix this:
1. after doing initial mutations, just divide sigma by 10 to get to the optimal start point. Would have to tune this hyperparameter though.
2. Update sigma mutation param, possibly changing up vs down. If we make it easier to mutate upward, what happens. We will come to equilibrium at a lower
spot. Could this be enough to fix? Would have to compare rest of the training too. And would probably make the increasing sigma thing be like 10x higher
3. Completely change the pretraining phase.

1. Graph the average angle of a minibatch gradient with the full training dataset gradient
2. Graph the average angle of a population sample step with the minibatch gradient
Presumably, both will be normal, and hopefully we can do some theory to determine average angle of GA
with the train gradient

Alternative option: experimentally determine the average angle of a best 256 from minibatch with 
training gradien

Consider two random normal vectors a and b. We want to determine the distribution of their cosine
similarity. Without loss of generality we let a lie along the x1 axis.
a = (a1, 0, 0, ..., 0)
b = (b1, b2, ..., bn)

Then their cosine similarity is 
(b1 * a1) / (||b|| * ||a||)
 =
 (b1 / ||b||) * (a1 / ||a||) = (b1 / ||b||) = (b1 / sqrt(sum(bi^2)))
Now, when the dimension n is large, sum(bi^2) can be approximated as 
var(N) * N = N
so this becomes
b1 / sqrt(N)
b1 is normally distributed with mean 0 and std 1, so what happens when we multiply it by 1/(sqrt(N))? 
The standard deviation is simply multiplied by 1/(sqrt(N)).
-> This matches experiments

Now what though?
If we assume the training gradient lies along x1 (without loss of generality), we assume the minibatch
gradient has a similarity sim with the training gradient, then we can further assume without loss of generality
that the minibatch gradient is any vector that has such a similarity (since they are all radially symmetric again
with respect to the training batch gradient). However, then for the population sample, with similarity s2
to the minibatch gradient, symmetry is broken and we need to look at the full distribution...
over all 


ALRIGHT what is the full experiment:
1. compute training data full gradient
2. compute a minibatch gradient
3. sample 256 vectors, take the one with best cosine similarity to minibatch gradient
4. measure its cosine similarity to the full gradient

And then I guess we want to compare this to just taking 256 vectors and getting the one with highest cosine
similarity to the training gradient

OK, we can see empirically that a small minibatch results in the best256 update often being worse wrt the training
distribution (about 1/4 of the time). How can we improve this? Let's try just combining the two best into one and 
see if that cancels out some of the noise:
Result: It does not, the distribution looks very similar.

Besides increasing minibatch size, not really sure what we can do here.
I suppose we can also increase the population size so the randomly drawn vector is closer to the minibatch direction
(which should work, since gradient descent works). I doubt this will work nearly well enough though since
we never really sample things directly along the gradient 




