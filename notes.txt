IDEA: what does the path of optimization look like? Does it double back? Are gradient
steps that are taken near the end harmfull if taken near the beginning, etc?

TODO:
test how training speed changes with size of network. Does it get harder to optimize
with GA when the search space is higher dimensional?
TODO investigate if it's better to normalize update magnitude every step, or if by
doing what we're doing now, we are achieving a benefit from smaller updates when we are less confident
and larger when we're more confident?

IDEA: 
generate completely random neural networks until we get a high-performing one on mnist.
If it generalizes, then we can say that gradient descent has no effect on generalization ability,
and instead it is probably due to the "number of generalizing networks" for 
certain data distributions.
One way to think about this would be compression - the maximally-compressed network has the most
"free variables" and hence takes up the most volume in the high-dimensional manifold of its parameters.

TODO: try, instead of finding the best mutation by evaling each mutation on the full trainset,
just randomly mutating and choosing the one that has highest cosing similarity with the gradient. See if it 
performs better / worse / same as full GA.

TODO include hyperparams in save .pkl 

List of main experiment ideas:

| TODO: try adding some loss term that enforces a particular order on the features learned,
| then see if we can interpolate between different networks trained with that norm
| to test if it works. Toward the eventual goal of allowing multiple modules to learn
| and interface together.
| 
| Add biases to GA mutation, figure out right mutation schemes
| 
| Continue to try smaller LR mutation rates to find the optimal one
| 
| Try LR mutation with a smaller generation (or fixed with higher generation) to isolate effects of each
| 
| Experiment with different types of mutation again to verify that exponential is performing better than normal
| 
| Experiment with different values for lambda in the exponential network
| 
| Instead of mutating parameters directly, mutate how they change with each minibatch (a hebbian rule, for example)

The really long run is NOT a multi, it is misnamed

3/13/24: working on adding sexual reproduction. Should speed up convergence since now beneficial mutations can occurr in parallel
(and be recombined into the same chromosome by sexual reproduction).
To compute the likelihood of two beneficial mutations A and B combining in the immediately following generation, you can do the math
like here:
https://www.wolframalpha.com/input?i=plot+1-%28%28x*%28x-1%29-2%29%2F%28x*%28x-1%29%29+%2B+%282+%2F+%28x*%28x-1%29%29%29*%283%2F4%29%29%5Ec.+let+c+%3D+512.+between+x+%3D+2+and+x+%3D+128
the 3/4 comes from the chance that, given parents A and B, a child does not receive both A and B (1/2 * 1/2)
ACTUALLY: on second thought, I don't think this math is important. There is a competing effect - the above shows that the fewer parents
in a generation, the more likely that two beneficial mutations will be immediately combined. BUT the opposite effect is that it is ALSO more likely
for a single beneficial mutation to be overwrote entirely by another beneficial one (A has 4 kids that are all better than B's kids, because A and B 
never combined, and there are only 4 parents, so A makes all 4 parents of the next generation). If there are MORE parents per generation,
then it is less likely that a beneficial mutation will go extinct before it can recombine with other better ones.
Can always just test this stuff experimentally anyway...


Current best idea:
test the hypothesis that GA mutations interfere with previous knowledge by memorizing 
some random dataset.

With these params:
input_dims = [64,64,3]
num_classes = 10
batch_size = 32
num_train_datapoints = 16 * batch_size
num_val_datapoints = 2 * batch_size
kernels = [8,4,3]
channels = [3,32,64,64]
strides = [4,2,1]
hidden_size = 512
adam gets to 1e-5 within ~100 epochs on MemorizationDataset


Hypothesis:
randomly mutating the entire network every time makes it difficult to develop complicated machinery,
since as we build up skills, every mutation has a chance of interfering with all previously built up 
skills.
We need a way for mutations on average to be benign, or at least a way for mutations to not
"overwrite" whatever skills we already have
Rapidfire brainstorming::
-Mutations apply to only one layer (skip connections)
-mutations add layers one at a time
-mutations add separate models that are ensembled together
-there is an attention layer of some kind that can attend to different mutation layers
-Setup the random weights such that layers on average make small changes
-each mutation is a totally separate model that additionally outputs an "importance" 
channel, and at every step we use the model that outputs the highest importance. This last one
feels somewhat physical, we have all these different desires and they often compete, and presumably
the most pressing one wins. The alternative idea is the central system has its own coherent 
narrative that it takes into account all subsystems and then choses.
Some problems:
1. how do we mutate the final layer?
--- Incredibly it seems like genetic algorithms may provide a solution to this already - 
over time, chromosomes that are most likely to be robust to mutation with respect to the fitness
function will out-compete others. Then, only the unimportant elements will mutate and hence discover
new fronteirs


Roms are installed: /Users/luke/Library/Python/3.9/lib/python/site-packages/AutoROM/roms


If I get this error when using eksctl:
Error: checking AWS STS access â€“ cannot get role ARN for current session: operation error STS: GetCallerIdentity, https response error StatusCode: 0, RequestID: , request send failed, Post "https://sts..amazonaws.com/": dial tcp: lookup sts..amazonaws.com: no such host
If I just specify the region to be us-west-2, then it works again.
TODO investigate


TODO: 
the "frames" reported in the GA paper are "game frames", not "training frames". 
So if we train on every 4th frame, make sure we are incrementing frame count by 4 each training
step. Not sure if I am doing this right now.

TODO: havent figured out how to connect to rabbitmq UI


52.52 was the last price before I stopped everything


Things to think about
1. indirect encoding - how can we ensure that beneficial mutations are re-used everywhere they are applicable
in the network etc?
2. Sexual selection - how can agents choose their mates?
3. Diversity - how can we enforce some diversity constraints?
   --- there may be a way to enforce this by penalizing the opposite - 
   organism that are too non-diverse die out
4. think about global competition - in real life, organism succeed by finding NEW niches, not by
dominating an existing one (diversifying). one idea is to compete *locally*, vs only similar solutions




The current network is 3 convolutions with channels 32,64,64
and strides 4,2,1
and then a MLP.

Suppose that our model consists of multiple sub-components, which can evolve independently, but which
are ensembled together at the end in some manner. 
How are we to ensemble them together?
Three possibilities I can think of so far:
1. each model outputs an action directly, and these actions are ensembled somehow
2. each model outputs only a state, which is ensembled somehow into an action by a final layer
   that itself must evolve independently (or possible is fixed in some way)
3. Each model outputs both a action AND a state, but somehow we use both or only the state.

Another idea: I could evolve to a different objective (modeling the environment, for example), and
then setup different components myself to each evolve (rather than requiring the multiple components to 
evolve to their individual tasks from scratch)
   - then 

The "final layer" must be something constant, that inherently is able to "attend" to the ensemble.
Then we can evolve each ensemble

Alright here we go:
There are evolved state representations and evolved reward functions, and a meta-controller.
State representations and reward functions are fixed for a given agent.
An agent is trained on multiple episodes, during which the meta-controller learns to use the state
representations to maximize reward. Meta-controller could be for example an RL loop



This is all too complicated. The hypothesis that I actually want to test is 
1. do genetic algorithms as they currently exist reach a "diminishing returns" point whereby further mutations
   interfere with previous learnings.


Current issue is that each generation takes successively longer to inialize the policy. 
This is a bottleneck for the memorization dataset, it may not be the bottleneck for a larger task 
(at generation 500 it takes 2 minutes to initialize the policy). No matter how many workers we scale up
to, whatever percent of the work-time it takes to initialize the policy will always take up that same
amount of resources (money or time). Possible solutions:
1. try to speed up the initialization process somehow
   - paper solution: pre-generate random matrix which we then index into? I'm not sure how this would be faster
2. try to prevent full re-initialization when possible? 
   - each worker can cache some set of networks, and then only has to update the network with just the new mutation.
     -- difficulty: how much space do networks take? can we store multiple on a worker?
     -- maybe we don't have to, if the master thread can organize the tasks so that workers usually receive 
        individuals that they already have cached. Seems tricky
3. change the way in which networks mutate, to be faster.
   - instead of mutating entire network, mutate one layer? reduces time by factor of (network depth). can we do this without
   increasing total generations required (or possibly even decreasing it?)
^^^ THE ABOVE IS SOLVED ^^^ (for some modules, not all)


Random (not really related) thought: What if where was a way to enforce a standard ordering on network parameters?
For example, order nodes by sum of incoming weights or something like that. Would this make it easier to mutate
parameters one layer at a time?
- there are ways, see Keller's paper and git re-basin. They all require some computation though.


Observation:
When we learn the no-grad memorization task, if we simply use the optimal settings for the normal 
memorization task (1 parent sigma 0.02 child 16), learning barely progresses at all. Increasing sigma
alone doesn't fix the problem, learning halts at around 21. However if we increase the number of parents
(and children) to 8(32), then learning continues to 15 at generation 500.
Hypothesis:
Something about this setup incentivizes parents to become robust to mutation with respect to the 
information they have already learned. Children of the parent that reproduces the highest average fitness
will be the most likely to populate the next generation. This means that parents that are "robust"
to mutation with respect to whatever they have already learned will be more likely to persist. However,
there is no incentive for them to develop robustness to mutation in generall - only robustness with
respect to the datapoints they have already memorized. So mutations will be able to continue
exploring new ground without overwriting previous accomplishments.



Possible next steps:
1. experiment with different architectures+mutation schemes, to find one that can solve the nograd memorization
problem quickly. A simple idea is just a NN thing where we mutate the 10 reference points, for example. 
3. run a bunch more experiments to see if the current setup can actually solve the nograd memorization problem
with some set of parameters
4. try to falsify our hypothesis that multiple parents allow multi-generation selection effects where 
chromosomes that are robust to mutation with respect to the current fitness level are more likely to survive
and propogate
5. Mutate mutation rate per parameter

What if we evolved a rule whereby connections between neurons were strengthened / weakened over time?
let's say fixed kernel size of 4: 
Neural network takes as input:
-- one window of the input (4x4xc)
-- current weights connecting each input to the output
-- (optional: current output activation)
And outputs:
-- updates to each weight
Could have other networks in there, for example one that takes as input all the channels at a given
location over multiple input images, and outputs an adjustment to all input weights etc

Random idea:
there are two elements of the brain.
The thing that senses
The thing that acts

-- If the thing that acts, can learn to operate with arbitrary types of senses, then the thing that senses
can evolve independently to just sense better (add more vision neurons, add more abstraction, add "memories" 
to the sensation if visual input triggers something encountered before, etc)
-- the thing that acts just has to be able to act with arbitrary sensual input (which it clearly must adapt
to over time, since it is arbitrary).


Random papers:
https://www.nature.com/articles/s41567-023-02332-9
-- mathematical modeling of both hebbian plasticy and random modifications reproduces natural 
   connection-strength distributions and clustering
-- in real brains, "fire-together-wire-together" is asymmetric and has time dependency. This paper
   doesn't look into that


We need a way for the genetic algorithm to learn which parameters to continue mutating and which parameters to 
stop mutating (This would also allow learning of the learning rate). 
Let's say we have n parameters, and we also keep track of n "mutation rate"s. 
Network mutatino would be p += normal(shape(p)) * sigma * (mutation rate)
The question is, how does mutation rate change?

I have this interesting idea that sometimes, mutations that CAUSE death could be beneficial to the population. Think
for example of this mutation rate above. For some parameters, we will want it to become very low, and then stay there.
But once it is low enough, there will no longer be any selective pressure on it - so it will random walk until it is high
again, and then it will take a lot of mutations to make it sufficiently low again. But, if the population were to develop
another mutation that causes future children to die if their mutation rate increases above a low threshold, then
it would be possible to NEVER see that parameter change again. 

Put more simply - if a mutation is slightly bad, it is to the population's advantage to weed it out early, before it 
randomly propogates (and consumes resources). This could be accomplished with some form of sexual selection also. ***

I think the first thing we should try is just the most basic "mutating learning rate". Keep a lr for each parameter, mutate
it at some constant rate, pass it through some function that maps it to [0,inf] (probably e^x), and then use it to mutate. Let's do it!


So far, attempts at variable learning rates have not allowed me to succesfully memorize 64 datapoints with the "tilldeath" loss.
One thought is that the population size is too small? If larger pop size, then non-harmful or slightly-harmful mutations
can persist longer before they are outcompeted and rendered extinct. Or just, there is more diversity in a given pop.
This doesn't really seem correct though - I don't want to just make it more likely that random chance will get us there,
I want to be able to learn what changes actually will.

What if we add some momentum to updates? If a param has been updated in one direction for the past few steps, then continue moving 
in that direction

Another thought is to add stochasticity into the actions - the loss can still be deterministic based on what actions are taken,
but this would allow agents with a higher likelihood for the correct action to be more likely to survive, and hence
provide some selective pressure to move in the right direction. A risk would be that too small populations will
now be dominated by luckiness in actions?

Apparently there is a "50/500 rule" in genetics (minimum viable population) that a population needs 50 individuals to prevent
inbreeding depression, and 500 to guard against genetic drift at large.

What stats should I record?
1. mean and variance of learning rate (across all params)
2. How much each "memory" is being used
3. magnitude of each "memory"
-- could just figure all this out from the save state

OK upon investigating, I found that the norm of each  memory was increasing with every mutation. Clearly this is undesireable,
increasing amounts of mutation are then required to make the same scale of changes. Possible fixes:
1. restrict the norm of each parameter to remain the same 
2. add some l2 regularization to the loss
3. multiple the mutation by the norm of the parameter
4. allow mutation size to increase in some other way
5. somehow restrict each mutation to keep the norm the same (similar to 1)
I like 1 the best, lets implement it.


For 1 though, how do we reverse the process (for efficient mutation from cached policies)?
given 
   noise
   p + noise / | p + noise |
and the knowledge that |p| = 1, how do we determine p?
- we can do it with the law of cosines, although I am not sure how to figure out the +/- in the
quadratic formula besides trial and error. Surely there is an easier way...


Alright after some random experimentation, I decided to try just mutating each "memory" to a random direction
on the unit sphere, without any regard to whatever was there previously. This learns reasonably fast (for the normal
memorization task), but not as fast as the previous memorization policy, which had the norm of each m emory increasing
with every mutation. I am not sure exactly why this is. One hypothesis is that, by adding to an existing memory,
we are making it more likely to stay "near" that direction, and random walking "near" a good direction is more likely
to uncover a better direction than random re-initialization. Another hypothesis is that it makes it possible for the memory 
to become more and more robust to mutation. I still want to figure out a more intelligent way to select which memory to mutate,
though.

The 'increasing norm' approach could have another benefit, which is that it would allow stuff like this

  +       +     +
     +   +       +
   +       -  +  
      +   +       +
    +   +   +

 to be classified with only 2 memories, if the magnitudes are selected carefully (I think). Not sure about this, TODO

Let's consider this solution again:
- memories are unit vectors
Somehow, memories determine logits for each action. The closer a memory is to a query, the higher its logit is.
Then, selection pressure at the population level will move each memory closer to the query.


What if we just scale up the number of memories a lot? Then we can randomly mutate to add in vectors that dont change much,
more likely to eventually get ones that are close to correct
TODO think about if this still has problems once we need to remember more things

The downside of this whole memory mechanism is the following:
suppose we have a bunch of dogs in an area with one cat
we can randomly mutate a couple memories in this "dog" area that classify the whole area as dog.
Then, how do we learn a single memory for the cat?
well, even if we randomly mutate to have exactly the cat vector, we may not learn it if it overwrites the 
correct "dog" classification for some other nearby vector. 
So we would be relying on random mutations to build up in this "dog" area over a long time, without any
selective pressure to do this, until it is "safe" for the cat memory to mutate because it will no longer
disturb the correct classification of any dog memories.

Experiment:
train some MemModuleBasic to -0.5 loss, which means there is a single datapoint incorrectly classified.
I tested how many memory modules are being used to classify the other 63 datapoints, and it is 63. This was initially
surprising to me, I thought there might be a couple that are classified correctly, but I think what is happening is that
none of the memories are actually that close to the datapoints, they are just closer than any other memory, and 
there are so many dimensions that it is keeps being possible to find some smaller and smaller subspace of the sphere
that is closest to the target and that doesn't interfere with any other memories. TODO this is something we could experimentally/mathematically
verify. So once we get to 63, the subspace that's left is very small (or possibly nonexistent, TODO check).
The experiment I tried here so far is to just take a memory that isn't being used, set it to exactly equal the desired target memory, and 
see what happens. What happens is that the total number of correctly classified memories drops massively from 63 to 37. 


Plan:
1. make memmodule that mutates each memory instead of totally replacing it, keeping the memory normalized
2. add some kind of intrinsic reward which rewards how close the closest memory is to the input
-> this should be enough to stop getting "stuck" at the end, I think, and learn all datapoints faster

TODO redo some experiments, I had done them with sexual instead of asexual by accident


Did the above, it works, can learn 64 with 64memheads pretty fast (500 generations was the fastest I saw)
Still occasionally will get stuck on the last step, I am guessing because each memory is only updated 1/64 * pop_size
per generation (so once for my current settings). curious what happens if we just update all the memories

Alright looking at some runs with the above and various sigmas, sigma=1 seems to be the best! Maybe sigma3 also good but 
I'm inclined to leave it at sigma 1

let's do a quick "mutate all memories at once" experiment -> doesn't seem to work very well

WE need to speed up the random mutation right now. The "add a random normal and then normalize" is difficult to invert
so I'm going to move to another scheme:
1. generate a random (invertible) rotation. We can do this for uniform rotations, not sure how to restrict them
Maybe just generate some basis set of rations (all pairs of 2 coordinate systems, seems kinda overkill and would take a long time)

Other ideas...
1. provide some mechanism for reducing the size of a chromosome ( mutations that remove old mutations )
 --- need some intrinsic incentive for this, 
     

Maybe let's just work on something else for a bit..

Tradeoff with sigma: small values of sigma allow fitness to increase faster for good memories. BUT they make it much harder for not-good
memories to improve to be useful (need to get there with a random walk, not really feasible)
-- potential solution is to sometimes mutate entirely new vectors (in addition to the small sigma mutations)

Current thoughts: could just move around in Rn space (not on sphere), but still normalize before computing distance, 
problem here is just to prevent exploding norms, could use a torus space

Also had a thought that could dynamically determine the step size for updates by looking at the moving average of recent step sizes,
and then increasing/decreasing by some power factor. The idea is if a lower learning rate is more likely to improve the
fitness, then updates that actually do improve the fitness will be more likely to have had a lower learning rate, and the
baseline will adjust


IMPORTANT OBSERVATION:
when we start with a randomly initialized memory bank, learning is much slower than when we start from 0.
I suspect this is because with a randomly initialized bank, there is a starting vector that is "close" to 
every datapoint, but the vector is probably labeled incorrectly. Then when mutating, you have to mutate
a vector that is closer than the randomly initialized vector (~1/64 chance), AND that is labeled correctly (1/10).
So only 1/640 mutations will improve things (10 generations). 
We also observe that when training from randomly initialized memories, the intrinsic fitness DECREASES over time. 
(with the random resetting of memories, not random perturbation)
This seems to mean that usually when mutating we are moving a close vector away (presumably to improve 
another vector some small amount)
What if instead of mutating target and memory at once, we mutate either or?


Basically, whenever we have an "incorrect" memory close to a datapoint, in order to improve we need to do one of two things:
1. randomly select this memory to mutate, and mutate the class to be correct
2. random mutate another memory even closer than this one (and class correct)
Basically, the closer an incorrect memory is, the harder it is to improve. there is a tradeoff here in that the faster memories
mutate, the easier it is to remove bad memories (and replace them), but the harder it is for our intrinsic fitness to take effect
and keep memories close to their correct values. Intrinsic fitness as we've defined it now actually makes this even worse,
since it moves even incorrect memories close to their datapoints.

Basically we need another intrinsic fitness which is "number of used memories." and we need to often mutate deletions.

three possibly ways to mutate:
1. select an unused memory, randomly mutate it, randomly assign a class
2. select a memory and delete it
3. select a used memory and randomly perturb it.
Eventually there could be mechanisms whereby the frequencies of each of these options can evolve too, but no need for that at the beginning.
TODO ive never tried just transmitting the full network state, it might be faster than regenerating all the time... 

Weirdly, when I look at how fast the intrinsic fitness increases with different values of sigma, the value of sigma that increases the 
intrinsic fitness the fastest is not the one that increases the overall fitness the fastest. Not sure what the hell to make of this

Alright I think it's time to just start working on the next step whatever that may be.

Brainstorming:
1. setup another toy environment, where we have to both memorize random noise as well as learn some function approx. 
i.e. learn cifar as well as memorize random.
--- downside - may take a long time to test this, we might need lots of datapoints for cifar.
Can we make a similar test that doesn't require so much data? Some small set of 32 datapoints that we know can't be
memorized by the memorization module?
--maybe a small dataset of images where we just need to count the ones or something like that and take mod 10
-- maybe something simpler, just ten "images" one for each class that can appear anywhere
-- could also just use mnist and evaluate on a subset of images each time for speed
   -> let's just start with this

TODO setup an aws instance so I can easily spin up more experiments in parallel
c7g.2xlarge seems to be similar to my macbook, cost 0.1$/hr

Can we visualize the gradient updates at every step and then see what sort of mutations could reproduce them?

What was my thought process running with higher pop... the the rational was, if we just eval on 64 datapoints every time,
the most fit individuals will always be the most lucky onesa, and luck is too likely to completely outcompete any other
beneficial mutations. In retrospect this could still be true even after increasing the pop size. It seems like if
half the pop is making it through every generation then there would still be a pretty large signal from just 
improving at the actual task rather than random chance, so not sure what's going on here...
Are there some conflicting effects?
- the higher percent of each generation that has children, the higher likelihood that "good" mutations will not
be overriden by random chance. But at the same time, the LOWER the proportion of the following generation will
contain mutations from this parent, so the more difficult it is for this beneficial mutation to survive.

This is assuming each descendant has 1 parent. If we have 2 parents per offspring, we can basically double
the number of offspring an individual can have while still keeping the pop the same size (but we half the 
influence on the offspring from each parent)
--- we could test our theories by doing GA like we are currently doing it, but also evaling on the entire set,
and then tracking which beneficial mutations get overriden by random chance over time. TODO

random idea, multiple populations evolving independently and occasionally recombining?

TODO add bias FFS, or if not you have to check that your non-bias implementation still works with gradient ascent


The question we want to answer is WHY GA gets stuck somewhere when trying to learn mnist. Experiments to try:

1. eval on larger amounts of mnist at a time. 
Result: larger amounts of eval -> better performance at convergence

2. eval on full mnist. Does it converge?

3. given a training with a sequence of mutations, eval each mutation on the full mnist set, and 
determine whether good mutations just stop being found, or if bad mutations are overwriting good
mutations at too fast a rate.

4. train with GD, make sure our architecture is all set up correctly.
6. Try to get some context for the mnist loss we are getting - what is a good loss?

5. train with GD, look at what the parameter updates look like, to gain ideas for how to mutate in GA
--- maybe simplify this to just a single fully connected layer and investigate that...

7. Check if we are generalizing at all (add a validation set on mnist)

8. We already tried lots of different values of sigma and didn't find it to be all that important


Ok let's order these. I think I want to do (4) first, it seems like the best sanity check
And I suppose there's no point in ordering the rest until I get results from 4, who knows what we might learn

Findings from (4):
1. normalization of mnist images is important (speeds up convergence a lot)
2. loss should go to 0. Currently with Adam 0.001, 4 epochs to 99% valset acc and 0.02 train loss.
With SGD lr 0.1, can take as few as 3 epochs to 99% acc and 0.03 val loss!
3. one epoch of batches of size 64 takes 937 batches, so 937 steps. So even if GA is as fast as 
SGD, it will still take about 6k generations to get to .02 (in 1k it should get to val loss of .07 and 
97% acc)

Next step is to look at the parameter updates that SGD is performing and see if we can glean anything
interesting thereby.

Investigating the gradients, they clearly fail the shapiro test for normality. What distribution(s)
should we use instead to approximate them?
Findings from (5):
gradient updates follow a loosely exponential distribution (for weights). When I approximate 
this distribution with GA, still doesn't work very well. 

Think the next step is to simplify further, go to a single MLP, and figure out the best way to do
GA (rather than just randomly using the normal distribution)

Generating ideas for ways to update:
1. update only a single layer at a time? 
2. update only a single parameter at a time? Still follow exponential distribution
3. Can actually learn an algorithm for updating the weights (complicated) 
   -- real life organisms probably do not encode all synaptic connections and weights in DNA,
      more likely they learn various rules for how to update synapses 
   how would this work: 1
   1. mutate the rule
   2. run network on a batch, use the rule to update weights for that single batch
   3. run again on that batch, see if improved after update. Organisms that improve the most
      reproduce
   4. This requires storing all the network weights
   5. But once the rule is learned, can use it in future attempts

There are two unknowns:
1. how much does the inaccurrate estimate of fitness due to one minibatch affect things?
   if we do this experiment and find that we are able to learn entirely, then what does this tell us?
   that somehow the best random mutation on a minibatch is significantly different than the gradient 
   direction for that minibatch, despite the random mutations being able to recover the gradient exactly.
   Seems unlikely - more likely that the way we are mutating is just very unlikely to uncover the correct
   gradient direction.
2. what is the best way to mutate parameters when we don't have a gradient?


Three possible failure modes:
1. It reaches a point where it stops generating updates that increase fitness. Possibly lr too high,
possibly loss landscape too sharp
2. Updates that increase fitness are being generated but not being passed down
3. updates that increase fitness are being generated and passed down, but updates that decrease fitness
are being passed down just as often.
4. I suppose a fourth possibility is that it continually increases fitness, but the increases are so 
small

1. can be tested by looking at a save, and trying to generate updates for all the seeds near the end.
If we can randomly generate one that increases fitness then we can negate 1. 
3. If we look at a late save and evaluate each mutation on the full set, and if fitness increases
and decreases, then we can verify 3
2. We can additionally look at a late save and see if we can generate an update that increases fitness
(with the params the training was using). 

We have weak evidence against (2) since when we increase the number of datapoints we eval fitness on,
it doesn't entirely fix the problem (this should make fitness on a batch more closely match global 
fitness, and hence they should be passed down)

Another thing we could try is to generate the actual gradient, and then try different sigmas with
that gradient. See if the sigma we are using is just too large at the end


OK after optimizing the eval on MNIST a lot, we can run experiments faster and this is what I've found:
- Loss does seem to approach 0 with exponential updates. Does not approach 0 with the "one" updates that I have tried so far.
- it is the case the higher sigma results in faster learning initially but slower learning later. 

-- seems like a no-brainer to implement some kind of lr mutation, to allow the algorithm to update the lr
as it goes.

-- Another idea is to actually figure out the ground truth optimal LR scheduling. How do we do this?
Do a trial run where at every fitness level, we compute the next generation at a few different learning
rates, and then we save the lr that has the best performing child. Then we can setup a lr schedule,
run a GA with that lr schedule, and use that as our baseline when implementing the lr mutation.
Let's just bite the bullet and implement this thing

Problem with the sigma updating is that it makes my efficient network reconstruction not work very well.

Initial results from the LR mutation show it to do better than most other schedules, at least early.
It doesn't do *that* much better than just keeping a constant 0.03 lr. And I am also curious if even 
lower constant learning rates will eventually surpass it

Another experiment we also need to try is trying the lambda mutation with the same generation size. Because
right now I'm using a higher generation count, which could explain why the curve is better.
Alternatively, try a fixed sigma at the higher parent count.

Hypothesis: the sigma mutation as I have implemented is decreasing sigma too quickly.
Supporting evidence: a constant .03 sigma converges much slower at the beginning, but faster at the end,
than my sigma mutation method.
Are there any other alternative possibilities? Sigma mutation could be optimal and just random variance in runs
causes this. Can just run a few trials
I think we can just assume this hypothesis is true for now. What we need to figure out is WHY it is decreasing
too quickly. 
Hypotheses:
1. The loss landscape is generally very sharp, with a few directions sharply increasing for a large update,
many more moderately increasing for small updates only, and most decreasing. This means that the optimal strategy
is often to just wait and keep rerolling updates until we hit the correct direction, and we want to have the largest
possibly magnitude when we do hit it. However in practice what will happen is that if we don't hit the best update in 
one generation, then the offspring with lower mutation rates will dominate.
--- test: plot the distribution of fitness levels after a single mutation at various learning rates for a given save.
          We should be able to verify emperically pretty easily if this is what is going on.

Idea - examine the distribution of weights connected to single neuron on a fully trained network

Something doesn't line up here - when I look at the distribution of fitness changes for a single 
mutation, they are almost entirely negative. But when I look at the average fitness graph,
it just goes straight up. Even best fitness just goes straight up, when only working with
a generation size of 16. So 1/16 mutations must increase fitness

I thought maybe I wasn't loading the elite, but this isn't the case...
Double check config? Make sure I'm mutating the right way still?
I did change the mutate code, but shouldnt have changed anything, just made it a method

We can try training full runs at various LR, and then check how the fitness changes for various lr
at set fitness levels for each one (see if loss landscape looks significantly different
for each LR)

Not sure what I was doing wrong yesterday, but when I fixed everything so that it worked with 
MP, now we are getting enough improvements to supply the monotonic improvement (when we use a low enough
learning rate)

Idea I had when waking up: we can just try the beginnign of a LOT of runs, to see how much
the seed affects things (dont have to run all the way to 50k when using the same parameters)


FIRST POST:
Genetic algorithms on MNIST 
Exploring genetic algorithms to evolve function approximators.

First, we show results for training on MNIST with SGD as a sort of baseline. Inspiration from this post
for training fast, but we have used SGD to keep it simple. Still converges to 99% very vast.


Now, let's see how genetic algorithms do. We begin by asexually evolving a population of 64 individuals, 
where the fitness is the cross entropy loss on the full MNIST training data. However to even begin, 
we need to figure out how we are going to mutate the parameters of the MLP. 



TODOs:
0. Add validation acc to tensorboard
1. Optimal generation size - loss improvement analysis
2. Making it work with smaller batch size


One thing we need to do right away: add an initial phase to learning where we tune the lr 
without actually mutating the network. How do we do this? 
-- lr_only generations param to the network where for the first lr_only generations, we only
   mutate the learning rate.


What do we actually want to do?
Initialize neural network randomly a bunch of times,
evalutae each random initialization,
mutate with a given learning rate multiplied or divided by some scaling factor,
evaluate the new mutation improvement,
set new base lr to be the one with the best improvement


The problem is none of this actually solves the problem that the way I am mutating the learning rate right now
does not give me the optimal learning rate for evolution.

If I just increase the population size, then the current mutation scheme will give me close to the optimal learning rate.
./co

With small pop, elites important since sometimes fitness does not improve at all
With large pop, can remove elites entirely

What if we keep the small pop but we slow down mutation rate for sigma?
Then a larger sigma has more chance to find a good mutation before being outcompeted by more consistent
smaller sigmas...


TODO: fix loss computation (divide by num examples not num batches) after we've matched performance

lambda89 IS importnat! need to investigate...
Probably what is going on here is that my initial sigma mutation only only works well when the starting sigma is smaller
than the optimal sigma. May need to actually test this hypothesis

When staring with sigma 0.5 and exponential, and 500 sigma only generations, performance is not optimal. 
Probably sigma is still too high here.



TODO
There is something I don't understand about the variance of training runs or some setting that is causing things to 
perform much differently than expected. Are the best runs just chance? 

TODO 
perhaps exponential does worse simply because there is a higher variance in the magnitude of the update - 
try normalizing before updating. 
Some quick spamming in the interpreter says this is unlikely. The param size is (28*28)*128 for l1 and (128*10) for l2 anyway

Try visualizing the distribution of the updates actually taken by GA... this seems insane it's not going to look anything
except normal

Try visualizing the improvement distribution for different types of updates (normal vs exponential), see if any information 
can be gleaned from that

Another thing I haven't looked at is if the true gradient with respect to the entire training distribution matches the
form of the smaller batches

Unknowns:
1. how much variance is there really in a single run with good starting parameters?
2. What are the best sigma mutation parameters
3. If we initialized sigma1 and sigma2 differently could we get better performance?
4. We need to establish 1 before determining if exponential truly is worse than normal

What we have neglecting is that we need more than just "one trial fast". We need "enough trials
to ignore natural variance" fast. 

Catch 22 - we want to run a bunch of trials to see what the variance is, 
but we want to run these with the best parameters
but we can't figure out the best parameters when there's so much variance

Looking at the best performing trials, they don't seem to be the ones with the best initializations
There is so much randomness involved 

I have discovered that my initial sigma mutations does not result in the optimal starting sigma. Now, there are a few levers we could try to fix this:
1. after doing initial mutations, just divide sigma by 10 to get to the optimal start point. Would have to tune this hyperparameter though.
2. Update sigma mutation param, possibly changing up vs down. If we make it easier to mutate upward, what happens. We will come to equilibrium at a lower
spot. Could this be enough to fix? Would have to compare rest of the training too. And would probably make the increasing sigma thing be like 10x higher
3. Completely change the pretraining phase.

1. Graph the average angle of a minibatch gradient with the full training dataset gradient
2. Graph the average angle of a population sample step with the minibatch gradient
Presumably, both will be normal, and hopefully we can do some theory to determine average angle of GA
with the train gradient

Alternative option: experimentally determine the average angle of a best 256 from minibatch with 
training gradien

Consider two random normal vectors a and b. We want to determine the distribution of their cosine
similarity. Without loss of generality we let a lie along the x1 axis.
a = (a1, 0, 0, ..., 0)
b = (b1, b2, ..., bn)

Then their cosine similarity is 
(b1 * a1) / (||b|| * ||a||)
 =
 (b1 / ||b||) * (a1 / ||a||) = (b1 / ||b||) = (b1 / sqrt(sum(bi^2)))
Now, when the dimension n is large, sum(bi^2) can be approximated as 
var(N) * N = N
so this becomes
b1 / sqrt(N)
b1 is normally distributed with mean 0 and std 1, so what happens when we multiply it by 1/(sqrt(N))? 
The standard deviation is simply multiplied by 1/(sqrt(N)).
-> This matches experiments

Now what though?
If we assume the training gradient lies along x1 (without loss of generality), we assume the minibatch
gradient has a similarity sim with the training gradient, then we can further assume without loss of generality
that the minibatch gradient is any vector that has such a similarity (since they are all radially symmetric again
with respect to the training batch gradient). However, then for the population sample, with similarity s2
to the minibatch gradient, symmetry is broken and we need to look at the full distribution...
over all 


ALRIGHT what is the full experiment:
1. compute training data full gradient
2. compute a minibatch gradient
3. sample 256 vectors, take the one with best cosine similarity to minibatch gradient
4. measure its cosine similarity to the full gradient

And then I guess we want to compare this to just taking 256 vectors and getting the one with highest cosine
similarity to the training gradient

OK, we can see empirically that a small minibatch results in the best256 update often being worse wrt the training
distribution (about 1/4 of the time). How can we improve this? Let's try just combining the two best into one and 
see if that cancels out some of the noise:
Result: It does not, the distribution looks very similar.

Besides increasing minibatch size, not really sure what we can do here.
I suppose we can also increase the population size so the randomly drawn vector is closer to the minibatch direction
(which should work, since gradient descent works). I doubt this will work nearly well enough though since
we never really sample things directly along the gradient 

What if each gene has a "importance" value that determines its likelihood to be dropped at the next generation?
When genes are first mutated they have low importance. Every time a gene is not dropped, its importance is 
raised.

Could have something where genes that have been lost are still tracked, and if a parent with a gene that
has been lost reproduces with a parent that still has the gene, the offspring doesn't have the gene.

We need to add multiple parents and some kind of sexual selection so the best mutation of each generation
can eventually outcompete the others.

Simplest possible place to start: just have 1/4 of pop reproduce, and their offspring has all of their genes.

Things to try:
1. see how often the optimal mutation wrt the training data is in the top 16 optimal mutations wrt the minibatch,
etc.
2. see if there is another more optimal way to estimate the gradient from a bunch of measurements, perhaps a weighted 
average? see zeroth-order estimation paper RGE
3. implement averaging into the evolution somehow - would need to encode additional information in the dna,
-- like, multiplier for update -- and then we can take this multiplier into account when adding mutations,
would allow us to effectively average the results from the top X of a single generation
4. Could test the hypothesis that averaging the top 64 results from a single generation would actually work
(like seen in the graph). This is probably the best place to start. If it doesn't work, we dont understand
something and we have to figure out why. This will require some coding -
probably easiest way to do it is to encode a weight with the dna like described in 3 above. But could
also implement full network passing and see if that is better.

Another TODO: try refactoring the way we run GA into one huge forward pass, like the colab.py in other,
and see if lets us iterate faster

Differences between colab.py and me:
1. runs multiple generations per batch. This may also be the key to their huge speedup
2. Averages the weights of the top 4 performing parents. This is similar to what I'm doing
right now, or at least I think it should be doing (except I'm doing 64 / 256)

Probably the learning rate is tuned incorrectly - we want a higher learning rate for the 
averaged updates, and a lower learning rate for the single mutations, right?
Or maybe I just have a bug? I'm unclear why we even needed to raise the learning rate at all...

TODO add some check for if an experiment is not improving so we can prematurely stop it so we don't waste
gpu time

WHY is the average update not working as well? Brainstorming:
1. learning rate isn't correct
-- need to check both the un-averaged update and the averaged update
   -- could add a check to gradient2 that actually updates the parameters
      to figure out the best gradient
2. cosine similarity with gradient isn't actually a good estimator of how to improve 
parameters? 
   -- TEST: can generate random perturbations, plot fitness improvement vs cosine-sim to gradient
3. perhaps the average update is still too small? 
   -- need to actually test the gradient
   -- could add something complicated to the update that records the old policy params,
      updates with all new ones, then normalizes the update. Would take a long time
4. Test addin gthe top 64 parents together and see what the magnitude is

Should we try the gradient2 experiment using actual random updates + evaluations
rather than simply checking the cosine similarity with the gradient?

When we just plot the improvements due to random mutation, the graph looks nothing like
the cosine similarity graphs of the gradient. 
<- we can double check this is working by plotting at an earlier checkpoint and making sure it shows
how we are improving TODO

So far:
1. I found a bug, wasn't scaling learning rate correctly. doesn't seem to help gradient3.py results though
2. Updated some spots where we were using the same dna list instead of copying it. Dont think this matters anywhere though

Should I do the experiment where I see how much the cosine similarity correlates with an update fitness improvement?
One with the average approach is that the "bad" mutations are always applied and never forgotten.
This isn't enough to explain the cosine similarity discrepancy though

TODO: understand why, after my bugfix of reducing the magnitude of the single gradient updates, convergence is way 
slower. Did I introduce some other bug?
Could just manually compute the norm of the update during normal training and see if it is increasing or decreasing
etc.

There may be a problem here where, the easiest minibatches result in the highest fitness, and so are always
over sampled during mutation. If this is the problem it also explains why the colab.py approach works.
Could also fix by evaling the current policy as well as the mutated policy on each mutation,
to get an estimate of the fitness improvement rather than the overall fitness.

ALRIGHT
im 90% sure that the problem is that minibatch fitness is too high variance. minibatch fitness ranges from 
0.2 to 0.7 (when ave fitness is 0.45), which will dominate any signal from perturbations. So we simply
take the mutations from the luckiest minibatches, regardless of whether they improve performance or not.

What other things could we do?
1. evaluate "difference in fitness" on a minibatch and rank by this. This doubles computational cost,
but possibly we could get away with a much smaller minibatch. Also it seems very unlikely to be physical
- I can't really think of any direct biological analog except maybe some approximation via sexual selection
2. larger minibatches - nonphysical, dont like
3. Evaluate mutations for multiple generations? Seems similar to using larger minibatches, not sure
what the benefit could be here. 
- the difference between larger minibatches is that we can mutate as we go. Seems like this would have
the same problems though, hard to disentangle mutation performance from minibatch performance.
4. What if the evaluation fn is changed? What if we eval "number until incorrect" - this would
introduce even more variance in minibatch performance, seems like it couldnt possibly help
5 colab.py evals on one minibatch per generation, definitely fixes problem but I dont like it too much for a couple
reasons - one is it's not physical, and the other is that it seems like using multiple minibatches
would give us more information per compute if we can figure out how to use them effectively

What if we do some kind of sexual reproduction where each individual is associated with its own minibatch
(experience) and each individual can evaluate some set of other individuals on its minibatch,
and selects the one that performs the best. Could do best-matching of some kind

The way im doing it now works initially because mutations cause large enough changes to overcome the 
noise from minibatch stochasticity. So another avenue would be trying to make the mutations more effective
on average. This seems like it would also necessitate a different loss function / evaluation phase though.

Doesn't realllly make sense to put it in policies, cuz we are going to need logic in eval anyway,
might as well just put it all in eval. There may be an optimization where it goes in policies
but I don't think we need to worry about that too much right now.

Things to do:
- profile the whole algorithm
- figure out why policy creation time increases still
- verify that creating the duplicate policy isn't too slow
  --- checked, it takes the same amount of time as the original, although I didn't check cloning time
- Should I try making some simpler minimalist code to make it easier to optimize?
- see if current codebase can reproduce best 'all' results from previously to verify 
  everything is still working nicely
- verify that averaged updates have the same magnitude as single updates, I am skeptical of 
  this because of how performance decreased when I thought I fixed a bug
  --- this was verified, the 8 multiplier is very accurate
- check if we can use higher lr for the averaged updates, since they are closer to 
  the actual gradient direction 
  --- checked, (tried x2), it's worse

The reason why the colab.py is so much faster is the minibatch size is so small, and it trains on 
the same minibatch for all mutations, so it is very rare to shuffle the dataset and we can easily prefetch
for the next generation.

I want to have each individual trained on a different minibatch - this is necessary when the individuals
collect their own data, as in acting in an environment. 
What we need to do is do some controlled experiments to see what exactly is making the algorithm take longer.

Is it
1. shuffling the data
2. transferring the data to the gpu
3. sending the data through the policy

The easiest thing to isolate is number 3 - just setup some massive random dataset on the gpu
and send it through the policy over and over again. 
No point brainstorming further until we test this! 

When simply loading the same data, already on the gpu, and sending it through the policy, this is the time
0.026282072067260742
When loading the same data starting on the cpu and adding ".to(device)", we get
0.04416513442993164
When I generate the whole dataset anew every tiem (on cpu) and then send to gpu, it takes
1.0842211246490479
When I generate on the gpu directly, it takes
0.0339810848236084 WOW that is fast
When I generate a tensor anew on the cpu every time, but send the same original tensor to the gpu, 
it takes
1.0541088581085205 
When I generate 100 batches on the cpu, and then yield a randomly indexed subset of that 
(on the cpu) and then transfer to the gpu, it takes about 1s again
1.073004961013794
When I compare doing this on mps vs cpu (with the same dataset size, which is using mult=20 on 
my laptop) I get speed
0.04650092124938965 < for cpu
0.03665781021118164 < for gpu
I think the 100 batches above is just slow because it doesn't all fit into ram. If this is 
correct then if we try 30 batches it should be fast again
Alright I'm pretty convinced this is a ram thing here, it goes fast if we just look at htop but if we start swiping between monitors it
goes slower (presumably as it has to use more ram? idk)

An entire population batch (500*256*28*28*4) takes 400MB
Note 500*256 / 50k = 2.56x as large as the full training data
The entire mninst dataset takes 179MB
with our usual num_workers = 8, this takes about 1GB, should fit comfortably in RAM

Things to try:
mating with improvement-weighted average rather than simply top 64

The ZO optimization paper, from what I can tell, got their graphs based on training for
a fixed number of epochs. So their graph doesn't show that RGE doesn't optimize, just 
that it optimizes slower (relative to the amount of training data).

List of unanswered questions:
1. Is there an alternative way to mate that's better than what I'm doing (perhaps weighted by fitness)
--- weighted by fitness is in fact better, although the weights i'm using are a bit ad-hoc
2. Will this method fail at higher dimensionalities?
--- tried some larger hidden dimensions and it worked with a lower learning rate, although it
    took longer as well
3. What exactly was causing my original implementation to be so much slower than other/min.py?
4. Is there any way to still perform well without computing fitness improvement due to mutation 
   on the same batch?
5. Will the current method work when using a non-differentiable objective?
--- TODO need to let this run a long time
6. Will it work with non-differentiable architecture?
7. Is my way faster or slower than fixed batch?
--- looks to be about the same
--- there may not be much advantage to running every environment with a different seed then. Could use
    same seed for each individual across a generation, then we wouldn't need to do this 
    fitness improvement scheme also, right?

When attempting to normalize the weights adjustment by layer, I think I am running into the following 
problem:
the first layer matters more than the second, so when we average the top 64, we get the a good direction
for the first layer and a meaningless noise for the second. This also means that the averaged meaningless
noise is smaller in magnitude. So if we normalize the update by each layer, we are amplifying this 
meaningless vector.

IDEA: If this "sampling from continuous and then evaling with discrete" tech works to optimize wrt 
accuracy only, then would it work to optimize an entirely discrete (and therefore non-differentiable) network?

some napkin math: brains evolved 500m years ago apparently. 
The fastest reproducing mammal (mice) takes about 1m per generation, that gives 6b generations
for modern brains to evolve?
Apparently humans mutate 100-200 times per genome

What if there are two stages of evolution:
1. a differentiable network that computes the loss given targets etc.
2. the main network
1 is optimized w.r.t. some non-differentiable objective. 2. is optimized wrt the differentiable one.


Currently we have two primary research directions:
1. How can we improve convergence speed for basic MNIST with cross entropy?
2. What changes need to be made to optimize effectively with non-differentiable objective (accuracy)?

TODO: implement "stop when val acc stops decreasing" into other/min.py

Brainstorming solutions to the non-differentiable objective:
1. evolve a differentiable approximation to the true objective, and then optimize wrt that. 
   Reminiscent of sexual selection - easy to evaluate "organism is bigger", which translates
   to higher probability of success, etc.
2. Some different mutation scheme - what if we allow increasing the network size, adding components
   etc? Seems unlikely that this would solve the problem but I suppose in the completely general case,
   the solution above is a subset of this one
3. perhaps just scaling up could solve? 
4. We could perhaps investigate WHY learning has stopped
   - mutations improve fitness too rarely
   - mutations decrease fitness too often
   - minibatch fitness is too decorrelated with true fitness
5. We can try optimizing wrt the full training data again instead of using minibatches
   --results: learning still flatlines at around 90% val acc

Here's what we're going to do
1. evolve a network that takes images as input, logits as input, and outputs loss 
2. train (sgd) a network to minimize loss of above

What is the actual problem with optimizing non-differentiable goals?
Suppose the loss curve looks like this
_
 |_
   |__
      |____________
                   |_______
The first couple of steps are easy to optimize. But after the third, there is no incentive to move right on the curve,
since it is flat. The way I have things implemented right now, it isn't even capable of random walking that way, since we 
always mutate from the previous best, and the best will be the first model on that level. 

After updating so it's allowed to random walk, doesn't seem to help much at all.

In the 1d case, a simple solution would be to just keep mutating in a given direction until fitness changes.
In the high dimensional case, I am not sure how to generalize this - we can try, but most directions will not
lead to decrease in fitness.
- we are impossibly likely to select the "true" direction. TODO formalize this? 
- although half of all directions are pointing slightly downward (the loss curve is locally a hyperplane), 
  most of these downward directions are "nearly perpendicular" to the actual downward gradient

Mantain a population of agents where we encourage diversity somehow?

If we do the stochastic optimization with a massive population, any chance that would work better?

TODO: why does a lower learning rate not work at all with sampled acc?

num_workers = 2 is faster than 1 on the aws instance but adding more isn't any faster (the cpu ram is already fully being used
at 2)

The aws instance is slightly slower than the macbook at the same settings. The main difference I see is the number of cpu cores - 
macbook has 8, aws instance has only 4. Is there a way to disable cores for a specific python thread and see if this is
the difference?

To make faster:
remove pin_memory, just send to device in the workers
remove tensorboard
add --ipc=host to docker invocation

Volatile GPU usage is still pretty low on aws instance, bottleneck has got to be data loading right?

When I use workers and don't pin_memory or to(device), it is way slower

Also make sure eval_every_generation is high enough to get an accurate metric - when it is low,
the workers can spend the eval time loading the data. 
Additionally this means that clearly the data loading is the bottleneck

Aha! It is important that the .expand() step happens on the gpu. It appears that otherwise we transfer
a huge tensor to the gpu.

This also makes it very clear that the shuffling of the data is the bottleneck
-- WAIT - it wasn't actually the shuffling, I just needed to use persistent_workers=True.
I guess this is because the MNIST dataset actually loads the whole thing into memory? Whereas if the 
workers themselves were just loading a csv and then loading datapoints at runtime, this wouldn't matter

Also just use a normal dataloader.

on the p3.2xlarge instance:

batch_size 256:
    512 popsize:
    .01, not fully using gpu
    2048 popsize:
    .025, not fully using gpu (intermittent 0s)


batch_size 500:
    with 256 popsize
    .015s per generation for default settings
    with 512 popsize
    .015s per generation for default settings <<< also these clearly are not using full GPU, utilization is low on nvidia-smi
    with 1024 popsize:
    .024s per generation
    with 4096 popsize:
    0.087s per generation
    8192 gives cuda OOM

batch_size 1000:
    with 512 popsize:
    .03 per generation << also not using full GPU

batch_size 2000:
    with 512 popsize:
    .068 per generation << also not using full GPU, appears to be dataloader issue

TODO check that image preprocessing is correct (and the same) with each loader type

NOTES:
the first batch from the dataloader takes longer (~0.2s). This is true even when we dont shuffle. 
Why is this? I'm guessing the workers just don't prefetch that batch?
-- whatever, seems marginal (10 generations every 120 generations so it's like a 8% slowdown...)

.044 torch dataloader vs .06 for my MPD (after fixes)

TODO check that our forwarding two inputs doesn't slow things down significantly

HMM OK, if we reduce the number of workers in our MPD then it is just as fast as torch dataloader, wtf
Yeah we can get even faster than train_dataloader (.010s with default settings)
OK seems like both loaders are the same speed when we run with 1 worker (and this is the fastest).
So multiple workers is actually slowing stuff down, I wonder why. Synchronization?

Also we still do drop to 0% utilization once in awhile


Alright now that we have reasonably fast training, we want to figure out what settings give the 
best results per unit time (on given compute)

Easiest way is to just run with a bunch of different settings and see how they all go
Alternatively, we could do some gradient analysis 
1. given a batch size and a population, what is the best estimate of the true gradient?
2. given a batch size, what is the best population size?

TODO: experiment with very few parents

Seems pretty essential to use large number of parents
possible that I just need to change learning rate for different numbers?
Maybe can fix by normalizing update

Is it time to just start running on atari? We have something that optimizes non-differentiable
objectives well. What are the differences between this paper and the original paper? 
-- not much, lol. Probably the only "novel" thing I've discovered is that we can optimize
wrt accuracy if we sample from the network distribution, which agent methods have been doing
for ages anyway.
- So I guess our next direction is to try to apply the above idea to binary networks etc and see if it works...

MNIST simple MLP - 4 layers, 4096 binary activations per layer. They way the layer works in the first paper 
(binarized neural networks) is they just multiply W by the activations like normal, except W is constrained
to be either 1 or -1. This produces some output (an integer output), which is then passed through a
batchnorm layer, and then binarized again with a hard_sigmoid + round layer (modified so that gradients can pass
through it).

Probably the first thing I should try before doing anything more complicated is just randomly flipping 
bits of W with some probability and mutating that way, and let the network output a probability for its
final layer so we can optimize wrt cross entropy anyway...

Currently, the problem is since we start at 0 we are never getting anywhere close to above the necessary threshold for internal activations.
-- above is fixed but still we are not learning.
My first attempt had much too high variance output logits, the network always classified things as 100% or 0%.
By decreasing the temperature a lot we fix this but we still aren't learning.

I want to spend some time thinking about how to optimize speed since this moves so much slower than our previous implementation.
The problem is, previously we were able to use einsum that never requires storing the intermediate multiplication product
before summing (it's doing matrix multiplication instead of elementwise multiplication + sum). We need to figure out a way 
to do this with bool

This is how fast we run on p4
gen 2519 | loss: 51.1249 | accuracy: 92.72% | elapsed time 61.1 | seconds per generation: 0.024

Let's see if binary is also that fast?

Binary right now has the torch.concat op the bottleneck. Doesn't help to preallocate the memory either.
(it is a 2x slowdown approximately just having this op in here.) 
Probably this is slow because it's concatting a huge vector, (1024x500x28*28). This definitely isn't necessary,
we can just keep track of two W, one for positive and one for negative, run them both and then sum them up



BEST SO FAR (75%):
temperature 0.8
lrs = [0.0001]
population_sizes = [1024]
num_parents_for_matings = [1]
batch_sizes = [500]
hidden_sizes=[128]
prefix = 'binary_test1'
fitness_weights=False
fitness_types = ['cross_entropy'] # 'cross_entropy','accuracy','sampled_acc'
load_from = ''
model_type = 'EvoBinarized'

BEST SO FAR (79%):
temperature 0.8
lrs = [0.0001 * 0.03]
hidden_sizes=[4096]
layers=1
-- this is expected ~20 bit flips per mutation

TODO NEXT:
1. Figure out how we can use the same architecture as the OG binarization paper. 
Probably we need to do multiple steps for higher populations (so we can fit
each step into memory).
-> decided to just do simple implementation using one elites
DONE 2. Debug the slowdown when using higher hidden_sizes (time explodes until only doing
one generation in between every eval)
3. Hyperparam search for the new architecture (temperature, learning rate, popsize, etc)
4. Implement mating (instead of just asexual) 
---- Random thought: the flip masks can be combined somehow and applied to the base 
     (rather than combining the mutated individuals directly)
     This allows us to set the threshold for a flipped connection. 

even just 1024 pop size with 4096x4096 weights does not fit into memory (17G) - actual 34G with x and notx.

Options:
1. Just use a smaller population size until it works. Let's real quick test what this could be?
--- on macbook it's 16
2. If we separate the population into batches, what are the difficulties with this:
--- if we want to do some kind of weighted averaging, we would need to store all batches
    weights + results anyway. They dont fit on the gpu so we'd need to move them to the cpu
    This is probably going to be slow although I am not 100% sure
--- could instead store the top X% of each batch. This seems like a fine enough solution,
    and we don't need to even implement it for awhile (for now we're just doing, elite reproduces only)
--- if we are doing elite reproduces only, is there much different from just using a small batch
    size and taking the elite every time? Probably not too much 

CONSENSUS: 
- just implement elites and then use the max batch size we can, see how this works.
Also need to implement "improvement", otherwise simply having a lucky batch can cause us to 
mutate in a bad direction.

TODO doesn't seem like using elites actually helps much at all even for small pops

TODO NEXT:
- load the latest checkpoint and experiment with hyperparams to see if we can get it to learn faster.
- verify that our elites implementation is actually doing something, I am skeptical since it didn't matter in earlier experiments
- if hyperparam tuning doesn't increase speed much, try mating
- if neither work, need to brainstorm more ideas

flip one at a time seems best! (at least at 50% acc). So let's just hard-implement that. That also makes mating a lot simpler

TODO NEXT:
smallest possible learning rate (1 bit change per mutation) still caps out at 70% acc. Need to investigate what the failure mode is here.
1. Can try this small learning rate on the single network to get an idea of what acc we should be expecting
2. Can try learning with larger batches (full training data) - or explore in some other way whether the problem is that
   we are updating the network with "bad" updates with respect to the full training data, despite them being improvements
   with respect to a single batch
3. There is another possibility - the network architecture is not amenable to single-bit updates at a certain point.
   Can investigate this - perhaps my activation function is bad, perhaps too many neurons are dieing. Would need to brainstorm this more

Indeed, when we try with larger batches performance keeps improving. Interestingly, though, the average acc across population seems to 
mostly level out even as the best validation acc (and loss) continues improving. 
- I suspect that most mutations decrease the acc by quite a lot, so this "decreased acc" is a dominating factor, which also probably has
pretty high variance. 
- This suggests we could perhaps speed up convergence if we don't try mutating connections that have already been tried - a simple implementation
of this could be randomly permuting all possible mutations, and just iterating through all of them one at a time, and then repeating.
- Actually this is unlikely to matter much for now, we just did 6k generations of 16  individuals for a total of 96000 mutations, but there are
~52million parameters here. I haven't done the full computation but this seems quite unlikely that we would be mutating the same parameter many
times in this timeframe.

What we should be doing is everything we can do optimize the training before we really scale it up. Ideas:
1. mating
2. batch norm
3. cuda kernel


TODO NEXT:
0. mating CHECK, it works, slight (20%) speedup
1. implement some kind of batch norm, see if it's better CHECK seems marginally better but probably not worth using
2. Is it time to bite the bullet and actually implement a cuda kernel?

Can we evaluate faster given that we only mutate one at a time? 
- if we cache results for each layer then we only have to compute after the current layer
- BUT we would have to cache results for every datapoint, seems impractical
additionally the layers after the mutation all need to be re-computed anyway 

Random note, jetsen nano developer kit is a small gpu can buy to use for cuda development, or robotics projects etc.

2060 has compute capability 7.5

Sometimes the CUDA error you are getting can be from pytorch incompatibility with the cuda version
-- perhaps install pytorch from source?

for pop size 64
num parents 1
batch size all
mb size 4096
hs 128
layers 1
elite True

Param count:  203264
Expected flips per mutation:  -203264
Eval time: 0.4555537700653076
gen 9 | loss: 2.6011 | accuracy: 11.62% | elapsed time 5.6 | seconds per generation: 0.568
Eval time: 0.4529104232788086
gen 18 | loss: 2.4774 | accuracy: 14.73% | elapsed time 11.1 | seconds per generation: 0.564
Eval time: 0.47159457206726074
gen 27 | loss: 2.3831 | accuracy: 17.90% | elapsed time 16.7 | seconds per generation: 0.564
Eval time: 0.4531440734863281
gen 36 | loss: 2.3115 | accuracy: 20.04% | elapsed time 22.2 | seconds per generation: 0.566
Eval time: 0.45537352561950684
gen 45 | loss: 2.2412 | accuracy: 22.53% | elapsed time 27.7 | seconds per generation: 0.566
Eval time: 0.4597043991088867
gen 54 | loss: 2.1705 | accuracy: 24.47% | elapsed time 33.3 | seconds per generation: 0.566
Eval time: 0.455824613571167
gen 63 | loss: 2.1056 | accuracy: 26.98% | elapsed time 38.9 | seconds per generation: 0.566
Eval time: 0.45394039154052734
gen 72 | loss: 2.0505 | accuracy: 29.02% | elapsed time 44.4 | seconds per generation: 0.567
Eval time: 0.458568811416626
gen 81 | loss: 1.9970 | accuracy: 31.06% | elapsed time 50.0 | seconds per generation: 0.566
Eval time: 0.4564549922943115
gen 90 | loss: 1.9407 | accuracy: 33.08% | elapsed time 55.5 | seconds per generation: 0.566
Eval time: 0.4547138214111328
gen 99 | loss: 1.8932 | accuracy: 35.34% | elapsed time 61.1 | seconds per generation: 0.567
Eval time: 0.46105527877807617
gen 108 | loss: 1.8511 | accuracy: 36.26% | elapsed time 66.6 | seconds per generation: 0.566
Eval time: 0.4657912254333496
gen 117 | loss: 1.8069 | accuracy: 39.00% | elapsed time 72.2 | seconds per generation: 0.567
Eval time: 0.4526040554046631
gen 126 | loss: 1.7733 | accuracy: 40.38% | elapsed time 77.8 | seconds per generation: 0.566


population_sizes = [2]
num_parents_for_matings = [1]
batch_sizes = ['all']
minibatch_size = 4096
hidden_sizes=[64] 
layers 1 
elite True
Param count:  101632
Expected flips per mutation:  -101632
Eval time: 0.46227121353149414
gen 120 | loss: 2.7856 | accuracy: 14.36% | elapsed time 5.5 | seconds per generation: 0.042
Eval time: 0.45279765129089355
gen 229 | loss: 2.4007 | accuracy: 15.36% | elapsed time 10.5 | seconds per generation: 0.042
Eval time: 0.4701356887817383
gen 338 | loss: 2.2811 | accuracy: 19.12% | elapsed time 15.6 | seconds per generation: 0.042
Eval time: 0.45237064361572266
gen 447 | loss: 2.1982 | accuracy: 21.82% | elapsed time 20.6 | seconds per generation: 0.042
Eval time: 0.44556093215942383
gen 556 | loss: 2.1021 | accuracy: 26.08% | elapsed time 25.6 | seconds per generation: 0.042
Eval time: 0.44850802421569824
gen 665 | loss: 2.0585 | accuracy: 28.34% | elapsed time 30.6 | seconds per generation: 0.042
Eval time: 0.4453911781311035
gen 774 | loss: 2.0070 | accuracy: 30.68% | elapsed time 35.6 | seconds per generation: 0.042
Eval time: 0.4454333782196045
gen 883 | loss: 1.9681 | accuracy: 32.51% | elapsed time 40.6 | seconds per generation: 0.042
Eval time: 0.4503941535949707
gen 992 | loss: 1.9139 | accuracy: 34.03% | elapsed time 45.6 | seconds per generation: 0.042
Eval time: 0.4450979232788086
gen 1101 | loss: 1.8435 | accuracy: 37.00% | elapsed time 50.6 | seconds per generation: 0.042
Eval time: 0.4585263729095459
gen 1210 | loss: 1.7918 | accuracy: 38.86% | elapsed time 55.6 | seconds per generation: 0.042
Eval time: 0.4545421600341797
gen 1319 | loss: 1.7546 | accuracy: 40.46% | elapsed time 60.6 | seconds per generation: 0.042


Alright, the plan is to implement a method to convert from EvoBinaryOptimzed to EvoBinary (and back). 
Then we will run both and see where they differ.

One inherent difference in the way I run both right now is how I pad the optimized one to
have input size a multiple of 64. I don't do this in the non-optimized one.

It appears that the input is not flipped but the weights are?

ALRIGHT we got the optimized version working, here is is with the following settings:
pop size 2 parents 1 batch sizes all mb size 4096 hidden sizes 64 layers 1 elite True
Param count:  1684
Expected flips per mutation:  -1684
Eval time: 0.0005500316619873047
gen 921 | loss: 1.9329 | accuracy: 31.20% | elapsed time 5.0 | seconds per generation: 0.005
Eval time: 0.000461578369140625
gen 1856 | loss: 1.5338 | accuracy: 50.71% | elapsed time 10.0 | seconds per generation: 0.005
Eval time: 0.0004589557647705078
gen 2793 | loss: 1.3486 | accuracy: 57.99% | elapsed time 15.0 | seconds per generation: 0.005
Eval time: 0.0004596710205078125
gen 3739 | loss: 1.2779 | accuracy: 61.15% | elapsed time 20.0 | seconds per generation: 0.005
Eval time: 0.00045752525329589844
gen 4686 | loss: 1.1867 | accuracy: 63.75% | elapsed time 25.0 | seconds per generation: 0.005

Optimization notes:
1. Somehow we need to schedule more warps
-- possibly I am allocating too much shared memory and so that's limiting the number of warps that
   can be scheduled at once, or some other reason, look into this 
   (see https://forums.developer.nvidia.com/t/increasing-number-of-active-warps-per-scheduler/195614)
2. Consider if I can rewrite code in some way such that it doesn't require as many synchronizations
3. Learn more about what a threadblock is, why do we actually want to have 32 threads in the y
   for each threadblock? Probably because they can share memory?

Things to look into:
1. global memory access is faster if the width of the array is a multiple of the warp size - look
   into this and see if we are satisfying this (see "Two-dimensional arrays" section under performance
   guidelines)
   - CHECK
2. branches - I think this should be fine, I never have if/else
   - CHECK
3. can __restrict__ be used on shared memory? 
4. look for type conversions between 32 bit and 64 bit? Seems like 64-bit isn't any faster than 32-bit 
   arithmetic so may be worth removing 64 bit stuff entirely
   - partial check, converted to int32 but should double check for all converions. This did not
   speed anything up
5. Try again to increase occupancy - if we have 2 blocks per SM, then one block can be doing
   compute while the other waits at synchronization barriers.
   whil
   don't have to wait at synchronization barriers
-- We need to decrease the block Y size by half so we can fit two. Then we need to add a loop
to loading the W into shared memory, since blockSize.y will be less than blockSize.x (which must stay 
at the warp size)

OK - we are limited by shared memory operations, which means we need to increase the amount of compute done per shared memory
operation. Consider what would happen if we double the output y-dimension and the output x-dimension.
This multiplies the number of outputs that a single block computes by 4, while it only multiplies the
amount of shared memory that needs to be loaded per block by 2. Note also that it reduces the total amount
of shared memory loads per kernel: we do not need to re-load an input row for each of the two outputs
in the original case (since they are both computed at once).



Getting errors with latest optimizations, also need to check that they actually work (run with python)

Alright, after we optimized shared mem -> registers -> compute, now the limit is the XU pipeline,
pretty sure this is just the popcount op (only shows MIO throttle on the popcount ops)

Moving back to int64 operations gives 120ms->117ms, not sure if significant. So it seems pretty clear
the bottleneck is the XU pipeline for the popcount op.
